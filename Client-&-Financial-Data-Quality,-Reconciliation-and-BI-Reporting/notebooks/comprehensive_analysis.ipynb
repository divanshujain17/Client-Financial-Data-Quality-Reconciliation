{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9711602",
   "metadata": {},
   "source": [
    "# Banking Data Quality & Reconciliation Analysis\n",
    "## A Comprehensive Learning Project\n",
    "\n",
    "**Objective**: Learn and demonstrate data quality assessment, reconciliation, and business intelligence using real banking data.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Data exploration and profiling techniques\n",
    "- Comprehensive data quality checks\n",
    "- Reconciliation methodologies\n",
    "- Customer analytics and segmentation\n",
    "- Data preparation for Power BI dashboards\n",
    "\n",
    "**Dataset Sources:**\n",
    "1. [Financial Transactions Dataset](https://www.kaggle.com/datasets/cankatsrc/financial-transactions-dataset)\n",
    "2. [Banking & Customer Transaction Data](https://www.kaggle.com/datasets/yogeshtekawade/banking-and-customer-transaction-data)\n",
    "3. [Transaction Data for Banking Operations](https://www.kaggle.com/datasets/ziya07/transaction-data-for-banking-operations)\n",
    "\n",
    "**My Approach**: This notebook shows my actual working process - I import libraries as I need them, work through problems step by step, and document my learning along the way. This is how I'd approach a real data analysis project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295c046",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Data Loading\n",
    "\n",
    "Let's start by loading the data. I'll import libraries as needed throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e06963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I need pandas to work with data and pathlib for file handling\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure pandas to show more readable output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"âœ“ Basic libraries loaded!\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb84d8c",
   "metadata": {},
   "source": [
    "### Load Customer Data\n",
    "\n",
    "**Learning Note**: Always start by understanding your master data (customers, products, etc.) before diving into transactional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ba1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project paths\n",
    "project_root = Path.cwd()\n",
    "datasets_path = project_root / 'Datasets'\n",
    "outputs_path = project_root / 'outputs'\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "outputs_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Load customer data\n",
    "# REPLACE 'customer_data.csv' with your actual filename\n",
    "try:\n",
    "    customer_df = pd.read_csv(datasets_path / 'customer_data.csv')\n",
    "    print(f\"âœ“ Loaded {len(customer_df):,} customer records\")\n",
    "    print(f\"âœ“ Columns: {list(customer_df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Customer file not found. Please download and place in Datasets/ folder\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # For learning purposes, I'll create sample data\n",
    "    # In real work, you'd use actual datasets\n",
    "    import numpy as np\n",
    "    \n",
    "    customer_df = pd.DataFrame({\n",
    "        'customer_id': range(1, 101),\n",
    "        'name': [f'Customer_{i}' for i in range(1, 101)],\n",
    "        'age': np.random.randint(18, 70, 100),\n",
    "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston'], 100),\n",
    "        'account_type': np.random.choice(['Savings', 'Checking', 'Premium'], 100),\n",
    "        'signup_date': pd.date_range('2020-01-01', periods=100, freq='3D')\n",
    "    })\n",
    "    print(f\"âœ“ Created sample dataset with {len(customer_df):,} records\")\n",
    "\n",
    "# Display first few rows\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5af41",
   "metadata": {},
   "source": [
    "### Load Transaction Data\n",
    "\n",
    "**Learning Note**: Transaction data is typically much larger than master data. Pay attention to memory usage and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data\n",
    "# REPLACE 'transactions.csv' with your actual filename\n",
    "try:\n",
    "    transaction_df = pd.read_csv(datasets_path / 'transactions.csv')\n",
    "    print(f\"âœ“ Loaded {len(transaction_df):,} transaction records\")\n",
    "    print(f\"âœ“ Columns: {list(transaction_df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Transaction file not found. Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Need numpy for random data generation\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create sample transaction data\n",
    "    n_transactions = 5000\n",
    "    transaction_df = pd.DataFrame({\n",
    "        'transaction_id': range(1, n_transactions + 1),\n",
    "        'customer_id': np.random.choice(customer_df['customer_id'], n_transactions),\n",
    "        'transaction_date': pd.date_range('2023-01-01', periods=n_transactions, freq='h'),\n",
    "        'amount': np.random.uniform(10, 5000, n_transactions).round(2),\n",
    "        'transaction_type': np.random.choice(['Deposit', 'Withdrawal', 'Transfer', 'Payment'], n_transactions),\n",
    "        'status': np.random.choice(['Completed', 'Completed', 'Completed', 'Pending', 'Failed'], n_transactions)\n",
    "    })\n",
    "    print(f\"âœ“ Created sample dataset with {len(transaction_df):,} records\")\n",
    "\n",
    "# Display first few rows\n",
    "transaction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9d4bb",
   "metadata": {},
   "source": [
    "### Initial Data Profiling\n",
    "\n",
    "**What I'm Learning**: Before any analysis, understand the structure, size, and basic statistics of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataset(df, name):\n",
    "    \"\"\"Generate a comprehensive profile of a dataset\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROFILE: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nðŸ“Š Dataset Overview:\")\n",
    "    print(f\"   Rows: {len(df):,}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nðŸ“‹ Column Data Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\nâš  Missing Values:\")\n",
    "        print(missing[missing > 0].sort_values(ascending=False))\n",
    "    else:\n",
    "        print(f\"\\nâœ“ No missing values found\")\n",
    "    \n",
    "    # Duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f\"\\nðŸ” Duplicates: {dup_count:,} ({dup_count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df.info()\n",
    "\n",
    "# Profile both datasets\n",
    "profile_dataset(customer_df, \"Customer Data\")\n",
    "profile_dataset(transaction_df, \"Transaction Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce794ca3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Data Quality Assessment\n",
    "\n",
    "**Learning Objective**: Implement comprehensive data quality checks following industry best practices.\n",
    "\n",
    "**Quality Dimensions:**\n",
    "1. **Completeness** - Are all required fields populated?\n",
    "2. **Uniqueness** - Are there duplicates where there shouldn't be?\n",
    "3. **Validity** - Do values conform to expected formats/ranges?\n",
    "4. **Consistency** - Are related fields logically consistent?\n",
    "5. **Accuracy** - Are there outliers or suspicious values?\n",
    "6. **Integrity** - Do foreign keys match primary keys?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef772ce",
   "metadata": {},
   "source": [
    "### 2.1 Completeness Check\n",
    "\n",
    "**What I'm Learning**: Measuring how complete your data is across all fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(df, dataset_name):\n",
    "    \"\"\"Analyze data completeness across all columns\"\"\"\n",
    "    \n",
    "    completeness_results = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        total_rows = len(df)\n",
    "        null_count = df[col].isnull().sum()\n",
    "        non_null_count = total_rows - null_count\n",
    "        completeness_pct = (non_null_count / total_rows) * 100\n",
    "        \n",
    "        # Check for empty strings in object columns\n",
    "        empty_count = 0\n",
    "        if df[col].dtype == 'object':\n",
    "            empty_count = (df[col] == '').sum()\n",
    "        \n",
    "        completeness_results.append({\n",
    "            'Column': col,\n",
    "            'Total_Rows': total_rows,\n",
    "            'Non_Null': non_null_count,\n",
    "            'Null_Count': null_count,\n",
    "            'Empty_Strings': empty_count,\n",
    "            'Completeness_%': round(completeness_pct, 2),\n",
    "            'Status': 'âœ“ Good' if completeness_pct >= 95 else ('âš  Fair' if completeness_pct >= 80 else 'âœ— Poor')\n",
    "        })\n",
    "    \n",
    "    completeness_df = pd.DataFrame(completeness_results)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETENESS ANALYSIS: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nAverage Completeness: {completeness_df['Completeness_%'].mean():.2f}%\")\n",
    "    \n",
    "    # Show problematic columns\n",
    "    issues = completeness_df[completeness_df['Completeness_%'] < 100]\n",
    "    if len(issues) > 0:\n",
    "        print(f\"\\nâš  Columns with Missing Data:\")\n",
    "        print(issues[['Column', 'Null_Count', 'Completeness_%', 'Status']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nâœ“ All columns are 100% complete!\")\n",
    "    \n",
    "    return completeness_df\n",
    "\n",
    "# Check completeness for both datasets\n",
    "customer_completeness = check_completeness(customer_df, \"Customer Data\")\n",
    "transaction_completeness = check_completeness(transaction_df, \"Transaction Data\")\n",
    "\n",
    "# Save results\n",
    "customer_completeness.to_csv(outputs_path / 'quality_customer_completeness.csv', index=False)\n",
    "transaction_completeness.to_csv(outputs_path / 'quality_transaction_completeness.csv', index=False)\n",
    "print(f\"\\nâœ“ Saved completeness reports to outputs folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af8cf9",
   "metadata": {},
   "source": [
    "### 2.2 Uniqueness Check\n",
    "\n",
    "**What I'm Learning**: Identifying duplicate records that could affect analysis accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df, dataset_name, key_columns=None):\n",
    "    \"\"\"Check for duplicate records\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DUPLICATE ANALYSIS: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Exact duplicates\n",
    "    exact_dups = df.duplicated().sum()\n",
    "    print(f\"\\nExact Duplicates: {exact_dups:,} ({exact_dups/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    if exact_dups > 0:\n",
    "        print(\"\\nâš  Sample of duplicate rows:\")\n",
    "        print(df[df.duplicated(keep=False)].head(10))\n",
    "    \n",
    "    # Key column duplicates\n",
    "    if key_columns:\n",
    "        for key_col in key_columns:\n",
    "            if key_col in df.columns:\n",
    "                key_dups = df.duplicated(subset=[key_col]).sum()\n",
    "                unique_count = df[key_col].nunique()\n",
    "                print(f\"\\nDuplicates in '{key_col}': {key_dups:,}\")\n",
    "                print(f\"Unique values: {unique_count:,}\")\n",
    "                \n",
    "                if key_dups > 0:\n",
    "                    print(f\"âš  {key_col} should be unique but has duplicates!\")\n",
    "    \n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'total_rows': len(df),\n",
    "        'exact_duplicates': int(exact_dups),\n",
    "        'duplicate_percentage': round(exact_dups/len(df)*100, 2)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Check duplicates\n",
    "customer_dup_results = check_duplicates(customer_df, \"Customer Data\", ['customer_id'])\n",
    "transaction_dup_results = check_duplicates(transaction_df, \"Transaction Data\", ['transaction_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dda49f",
   "metadata": {},
   "source": [
    "### 2.3 Referential Integrity Check\n",
    "\n",
    "**What I'm Learning**: Ensuring relationships between tables are valid (foreign keys exist in master tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aae281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_referential_integrity(transaction_df, customer_df):\n",
    "    \"\"\"Validate foreign key relationships\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REFERENTIAL INTEGRITY CHECK\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if customer_id exists in both tables\n",
    "    if 'customer_id' not in transaction_df.columns or 'customer_id' not in customer_df.columns:\n",
    "        print(\"âš  customer_id column not found in both tables\")\n",
    "        return None\n",
    "    \n",
    "    # Get unique customer IDs from each dataset\n",
    "    txn_customers = set(transaction_df['customer_id'].dropna().unique())\n",
    "    master_customers = set(customer_df['customer_id'].dropna().unique())\n",
    "    \n",
    "    # Find orphaned transactions\n",
    "    orphaned_ids = txn_customers - master_customers\n",
    "    orphaned_txn_count = transaction_df[transaction_df['customer_id'].isin(orphaned_ids)].shape[0]\n",
    "    \n",
    "    print(f\"\\nTotal Transactions: {len(transaction_df):,}\")\n",
    "    print(f\"Unique Customers in Master: {len(master_customers):,}\")\n",
    "    print(f\"Unique Customers in Transactions: {len(txn_customers):,}\")\n",
    "    print(f\"\\nOrphaned Customer IDs: {len(orphaned_ids):,}\")\n",
    "    print(f\"Orphaned Transactions: {orphaned_txn_count:,}\")\n",
    "    \n",
    "    integrity_score = (1 - orphaned_txn_count/len(transaction_df)) * 100\n",
    "    print(f\"\\n{'âœ“' if integrity_score >= 95 else 'âš '} Referential Integrity Score: {integrity_score:.2f}%\")\n",
    "    \n",
    "    if orphaned_txn_count > 0:\n",
    "        print(f\"\\nâš  WARNING: {orphaned_txn_count} transactions have no matching customer!\")\n",
    "        print(\"Sample orphaned customer IDs:\", list(orphaned_ids)[:10])\n",
    "    \n",
    "    return {\n",
    "        'total_transactions': len(transaction_df),\n",
    "        'orphaned_customer_ids': len(orphaned_ids),\n",
    "        'orphaned_transactions': orphaned_txn_count,\n",
    "        'integrity_score': round(integrity_score, 2)\n",
    "    }\n",
    "\n",
    "# Check integrity\n",
    "integrity_results = check_referential_integrity(transaction_df, customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76da7a9",
   "metadata": {},
   "source": [
    "### 2.4 Outlier Detection\n",
    "\n",
    "**What I'm Learning**: Using statistical methods (IQR) to identify unusual values that might indicate data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For statistical calculations, I need numpy\n",
    "import numpy as np\n",
    "\n",
    "def detect_outliers(df, dataset_name, numeric_columns=None):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OUTLIER DETECTION: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if numeric_columns is None:\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    outlier_results = []\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Calculate IQR\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Find outliers\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_pct = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        outlier_results.append({\n",
    "            'Column': col,\n",
    "            'Mean': round(df[col].mean(), 2),\n",
    "            'Median': round(df[col].median(), 2),\n",
    "            'Std': round(df[col].std(), 2),\n",
    "            'Min': round(df[col].min(), 2),\n",
    "            'Max': round(df[col].max(), 2),\n",
    "            'Q1': round(Q1, 2),\n",
    "            'Q3': round(Q3, 2),\n",
    "            'Lower_Bound': round(lower_bound, 2),\n",
    "            'Upper_Bound': round(upper_bound, 2),\n",
    "            'Outlier_Count': outlier_count,\n",
    "            'Outlier_%': round(outlier_pct, 2)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Range: [{df[col].min():.2f}, {df[col].max():.2f}]\")\n",
    "        print(f\"  Normal Range (IQR): [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        print(f\"  Outliers: {outlier_count} ({outlier_pct:.2f}%)\")\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_results)\n",
    "    return outlier_df\n",
    "\n",
    "# Detect outliers in transaction amounts\n",
    "amount_cols = [col for col in transaction_df.columns if 'amount' in col.lower() or 'balance' in col.lower()]\n",
    "if amount_cols:\n",
    "    outlier_results = detect_outliers(transaction_df, \"Transaction Data\", amount_cols)\n",
    "    outlier_results.to_csv(outputs_path / 'quality_outlier_analysis.csv', index=False)\n",
    "else:\n",
    "    print(\"No amount columns found for outlier detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc390b5e",
   "metadata": {},
   "source": [
    "### 2.5 Data Quality Summary Dashboard\n",
    "\n",
    "**What I'm Learning**: Combining multiple quality checks into a single executive summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quality_summary(completeness_df, dup_results, integrity_results, outlier_df=None):\n",
    "    \"\"\"Generate overall quality summary\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    # Completeness\n",
    "    avg_completeness = completeness_df['Completeness_%'].mean()\n",
    "    summary_data.append({\n",
    "        'Quality_Dimension': 'Completeness',\n",
    "        'Score': round(avg_completeness, 2),\n",
    "        'Status': 'âœ“ Good' if avg_completeness >= 95 else ('âš  Fair' if avg_completeness >= 80 else 'âœ— Poor'),\n",
    "        'Details': f\"{len(completeness_df[completeness_df['Completeness_%'] < 100])} columns incomplete\"\n",
    "    })\n",
    "    \n",
    "    # Uniqueness\n",
    "    dup_score = 100 - dup_results['duplicate_percentage']\n",
    "    summary_data.append({\n",
    "        'Quality_Dimension': 'Uniqueness',\n",
    "        'Score': round(dup_score, 2),\n",
    "        'Status': 'âœ“ Good' if dup_score >= 95 else ('âš  Fair' if dup_score >= 90 else 'âœ— Poor'),\n",
    "        'Details': f\"{dup_results['exact_duplicates']} duplicates found\"\n",
    "    })\n",
    "    \n",
    "    # Referential Integrity\n",
    "    if integrity_results:\n",
    "        integrity_score = integrity_results['integrity_score']\n",
    "        summary_data.append({\n",
    "            'Quality_Dimension': 'Referential Integrity',\n",
    "            'Score': round(integrity_score, 2),\n",
    "            'Status': 'âœ“ Good' if integrity_score >= 95 else ('âš  Fair' if integrity_score >= 85 else 'âœ— Poor'),\n",
    "            'Details': f\"{integrity_results['orphaned_transactions']} orphaned records\"\n",
    "        })\n",
    "    \n",
    "    # Consistency (Outliers)\n",
    "    if outlier_df is not None and not outlier_df.empty:\n",
    "        avg_outlier_pct = outlier_df['Outlier_%'].mean()\n",
    "        outlier_score = 100 - avg_outlier_pct\n",
    "        summary_data.append({\n",
    "            'Quality_Dimension': 'Consistency',\n",
    "            'Score': round(outlier_score, 2),\n",
    "            'Status': 'âœ“ Good' if outlier_score >= 95 else ('âš  Fair' if outlier_score >= 85 else 'âœ— Poor'),\n",
    "            'Details': f\"{avg_outlier_pct:.2f}% outliers on average\"\n",
    "        })\n",
    "    \n",
    "    # Overall Score\n",
    "    overall_score = np.mean([item['Score'] for item in summary_data])\n",
    "    summary_data.append({\n",
    "        'Quality_Dimension': 'ðŸ“Š OVERALL QUALITY',\n",
    "        'Score': round(overall_score, 2),\n",
    "        'Status': 'âœ“ Good' if overall_score >= 90 else ('âš  Fair' if overall_score >= 75 else 'âœ— Poor'),\n",
    "        'Details': f\"Average across {len(summary_data)} dimensions\"\n",
    "    })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DATA QUALITY SUMMARY\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Generate summary\n",
    "quality_summary = create_quality_summary(\n",
    "    transaction_completeness, \n",
    "    transaction_dup_results, \n",
    "    integrity_results,\n",
    "    outlier_results if 'outlier_results' in locals() else None\n",
    ")\n",
    "\n",
    "quality_summary.to_csv(outputs_path / 'data_quality_summary.csv', index=False)\n",
    "print(f\"\\nâœ“ Quality summary saved to outputs folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55639250",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Reconciliation Analysis\n",
    "\n",
    "**Learning Objective**: Implement reconciliation frameworks to identify variances between systems or time periods.\n",
    "\n",
    "**Key Concepts:**\n",
    "- System-to-system reconciliation\n",
    "- Period-over-period analysis\n",
    "- Variance calculation and thresholds\n",
    "- Exception management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53567457",
   "metadata": {},
   "source": [
    "### 3.1 Period-Over-Period Analysis\n",
    "\n",
    "**What I'm Learning**: Analyzing trends and detecting unusual changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_over_period_analysis(df, date_col, amount_col, period='M'):\n",
    "    \"\"\"\n",
    "    Analyze trends and variances over time periods\n",
    "    \n",
    "    Parameters:\n",
    "    - period: 'D' (day), 'W' (week), 'M' (month), 'Q' (quarter), 'Y' (year)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PERIOD-OVER-PERIOD ANALYSIS (Period: {period})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Create period column\n",
    "    df_copy = df.copy()\n",
    "    df_copy['period'] = df_copy[date_col].dt.to_period(period)\n",
    "    \n",
    "    # Aggregate by period\n",
    "    period_summary = df_copy.groupby('period').agg({\n",
    "        amount_col: ['count', 'sum', 'mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    period_summary.columns = ['Period', 'Transaction_Count', 'Total_Amount', 'Avg_Amount', 'Min_Amount', 'Max_Amount']\n",
    "    period_summary['Period'] = period_summary['Period'].astype(str)\n",
    "    \n",
    "    # Calculate previous period values\n",
    "    period_summary['Prev_Total_Amount'] = period_summary['Total_Amount'].shift(1)\n",
    "    period_summary['Prev_Transaction_Count'] = period_summary['Transaction_Count'].shift(1)\n",
    "    \n",
    "    # Calculate variances\n",
    "    period_summary['Amount_Variance'] = period_summary['Total_Amount'] - period_summary['Prev_Total_Amount']\n",
    "    period_summary['Count_Variance'] = period_summary['Transaction_Count'] - period_summary['Prev_Transaction_Count']\n",
    "    \n",
    "    # Calculate percentage changes\n",
    "    period_summary['Amount_Change_%'] = np.where(\n",
    "        period_summary['Prev_Total_Amount'] != 0,\n",
    "        (period_summary['Amount_Variance'] / period_summary['Prev_Total_Amount']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    period_summary['Count_Change_%'] = np.where(\n",
    "        period_summary['Prev_Transaction_Count'] != 0,\n",
    "        (period_summary['Count_Variance'] / period_summary['Prev_Transaction_Count']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Flag significant changes (>10%)\n",
    "    period_summary['Significant_Change'] = np.where(\n",
    "        np.abs(period_summary['Amount_Change_%']) > 10,\n",
    "        'âš  YES',\n",
    "        'No'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLast 12 Periods:\")\n",
    "    print(period_summary[['Period', 'Transaction_Count', 'Total_Amount', \n",
    "                          'Amount_Change_%', 'Significant_Change']].tail(12).to_string(index=False))\n",
    "    \n",
    "    return period_summary\n",
    "\n",
    "# Perform monthly analysis\n",
    "if 'transaction_date' in transaction_df.columns and 'amount' in transaction_df.columns:\n",
    "    monthly_trends = period_over_period_analysis(transaction_df, 'transaction_date', 'amount', 'M')\n",
    "    monthly_trends.to_csv(outputs_path / 'reconciliation_monthly_trends.csv', index=False)\n",
    "    print(f\"\\nâœ“ Monthly trends saved to outputs folder\")\n",
    "else:\n",
    "    print(\"âš  Required columns not found for period analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3437ce4",
   "metadata": {},
   "source": [
    "### 3.2 System-to-System Reconciliation\n",
    "\n",
    "**What I'm Learning**: Comparing data from different sources to identify discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_systems(source_df, target_df, group_cols, amount_col, \n",
    "                      source_name='Source', target_name='Target', tolerance_pct=1):\n",
    "    \"\"\"\n",
    "    Reconcile two datasets by comparing aggregated values\n",
    "    \n",
    "    Parameters:\n",
    "    - tolerance_pct: Acceptable variance threshold (default 1%)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SYSTEM RECONCILIATION: {source_name} vs {target_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Aggregate source\n",
    "    source_agg = source_df.groupby(group_cols)[amount_col].agg([\n",
    "        ('count', 'count'),\n",
    "        ('sum', 'sum'),\n",
    "        ('mean', 'mean')\n",
    "    ]).reset_index()\n",
    "    source_agg.columns = group_cols + [f'{source_name}_Count', f'{source_name}_Sum', f'{source_name}_Avg']\n",
    "    \n",
    "    # Aggregate target\n",
    "    target_agg = target_df.groupby(group_cols)[amount_col].agg([\n",
    "        ('count', 'count'),\n",
    "        ('sum', 'sum'),\n",
    "        ('mean', 'mean')\n",
    "    ]).reset_index()\n",
    "    target_agg.columns = group_cols + [f'{target_name}_Count', f'{target_name}_Sum', f'{target_name}_Avg']\n",
    "    \n",
    "    # Merge\n",
    "    recon = source_agg.merge(target_agg, on=group_cols, how='outer', indicator=True)\n",
    "    recon = recon.fillna(0)\n",
    "    \n",
    "    # Calculate variances\n",
    "    recon['Count_Variance'] = recon[f'{source_name}_Count'] - recon[f'{target_name}_Count']\n",
    "    recon['Sum_Variance'] = recon[f'{source_name}_Sum'] - recon[f'{target_name}_Sum']\n",
    "    \n",
    "    # Variance percentage\n",
    "    recon['Variance_%'] = np.where(\n",
    "        recon[f'{target_name}_Sum'] != 0,\n",
    "        (recon['Sum_Variance'] / recon[f'{target_name}_Sum']) * 100,\n",
    "        np.where(recon[f'{source_name}_Sum'] != 0, 100, 0)\n",
    "    )\n",
    "    \n",
    "    # Exception flagging\n",
    "    recon['Exception'] = np.where(\n",
    "        np.abs(recon['Variance_%']) > tolerance_pct,\n",
    "        'âš  EXCEPTION',\n",
    "        'âœ“ OK'\n",
    "    )\n",
    "    \n",
    "    # Match status\n",
    "    recon['Match_Status'] = recon['_merge'].map({\n",
    "        'both': 'Matched',\n",
    "        'left_only': f'Only in {source_name}',\n",
    "        'right_only': f'Only in {target_name}'\n",
    "    })\n",
    "    recon = recon.drop('_merge', axis=1)\n",
    "    \n",
    "    # Summary\n",
    "    total_groups = len(recon)\n",
    "    exceptions = len(recon[recon['Exception'] == 'âš  EXCEPTION'])\n",
    "    matched = len(recon[recon['Match_Status'] == 'Matched'])\n",
    "    \n",
    "    print(f\"\\nTotal Groups: {total_groups}\")\n",
    "    print(f\"Matched: {matched} ({matched/total_groups*100:.2f}%)\")\n",
    "    print(f\"Exceptions: {exceptions} ({exceptions/total_groups*100:.2f}%)\")\n",
    "    print(f\"\\nTotal {source_name}: ${recon[f'{source_name}_Sum'].sum():,.2f}\")\n",
    "    print(f\"Total {target_name}: ${recon[f'{target_name}_Sum'].sum():,.2f}\")\n",
    "    print(f\"Total Variance: ${recon['Sum_Variance'].sum():,.2f}\")\n",
    "    \n",
    "    # Show top exceptions\n",
    "    if exceptions > 0:\n",
    "        print(f\"\\nâš  Top 10 Exceptions by Variance %:\")\n",
    "        top_exceptions = recon[recon['Exception'] == 'âš  EXCEPTION'].sort_values(\n",
    "            'Variance_%', key=abs, ascending=False\n",
    "        ).head(10)\n",
    "        print(top_exceptions[group_cols + ['Variance_%', 'Sum_Variance', 'Match_Status']].to_string(index=False))\n",
    "    \n",
    "    return recon\n",
    "\n",
    "# Example: Simulate two systems by splitting transaction data\n",
    "# In real scenarios, you'd load from different sources\n",
    "if len(transaction_df) > 100:\n",
    "    print(\"\\nSimulating System A vs System B (for demonstration)...\")\n",
    "    \n",
    "    # Create two \"systems\" by splitting data or adding small variations\n",
    "    system_a = transaction_df.sample(frac=0.8, random_state=42)\n",
    "    system_b = transaction_df.sample(frac=0.8, random_state=123)\n",
    "    \n",
    "    # Find a grouping column\n",
    "    group_col = None\n",
    "    for col in ['transaction_type', 'type', 'category', 'status']:\n",
    "        if col in transaction_df.columns:\n",
    "            group_col = col\n",
    "            break\n",
    "    \n",
    "    if group_col and 'amount' in transaction_df.columns:\n",
    "        reconciliation = reconcile_systems(\n",
    "            system_a, system_b, \n",
    "            group_cols=[group_col],\n",
    "            amount_col='amount',\n",
    "            source_name='System_A',\n",
    "            target_name='System_B',\n",
    "            tolerance_pct=1\n",
    "        )\n",
    "        \n",
    "        reconciliation.to_csv(outputs_path / 'reconciliation_system_comparison.csv', index=False)\n",
    "        print(f\"\\nâœ“ Reconciliation saved to outputs folder\")\n",
    "    else:\n",
    "        print(\"âš  Required columns not found for system reconciliation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd2257",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Customer Analytics & Segmentation\n",
    "\n",
    "**Learning Objective**: Extract business insights and segment customers based on behavior.\n",
    "\n",
    "**Key Techniques:**\n",
    "- RFM Analysis (Recency, Frequency, Monetary)\n",
    "- Customer segmentation\n",
    "- Transaction pattern analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d6e1e",
   "metadata": {},
   "source": [
    "### 4.1 RFM Analysis\n",
    "\n",
    "**What I'm Learning**: RFM is a proven method for customer segmentation based on transaction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rfm_analysis(transaction_df, customer_df, reference_date=None):\n",
    "    \"\"\"\n",
    "    Perform RFM (Recency, Frequency, Monetary) Analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RFM ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    if 'customer_id' not in transaction_df.columns or 'amount' not in transaction_df.columns:\n",
    "        print(\"âš  Required columns not found\")\n",
    "        return None\n",
    "    \n",
    "    # Set reference date (latest transaction date)\n",
    "    if reference_date is None:\n",
    "        if 'transaction_date' in transaction_df.columns:\n",
    "            transaction_df['transaction_date'] = pd.to_datetime(transaction_df['transaction_date'])\n",
    "            reference_date = transaction_df['transaction_date'].max()\n",
    "        else:\n",
    "            reference_date = pd.Timestamp.now()\n",
    "    \n",
    "    print(f\"Reference Date: {reference_date.date()}\")\n",
    "    \n",
    "    # Calculate RFM metrics\n",
    "    rfm_data = []\n",
    "    \n",
    "    for customer_id in transaction_df['customer_id'].unique():\n",
    "        customer_txns = transaction_df[transaction_df['customer_id'] == customer_id]\n",
    "        \n",
    "        # Recency: days since last transaction\n",
    "        if 'transaction_date' in transaction_df.columns:\n",
    "            last_txn_date = customer_txns['transaction_date'].max()\n",
    "            recency = (reference_date - last_txn_date).days\n",
    "        else:\n",
    "            recency = 0\n",
    "        \n",
    "        # Frequency: number of transactions\n",
    "        frequency = len(customer_txns)\n",
    "        \n",
    "        # Monetary: total transaction value\n",
    "        monetary = customer_txns['amount'].sum()\n",
    "        \n",
    "        rfm_data.append({\n",
    "            'customer_id': customer_id,\n",
    "            'Recency': recency,\n",
    "            'Frequency': frequency,\n",
    "            'Monetary': monetary\n",
    "        })\n",
    "    \n",
    "    rfm_df = pd.DataFrame(rfm_data)\n",
    "    \n",
    "    # Create RFM scores (1-5 scale, 5 being best)\n",
    "    rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'], q=5, labels=[5, 4, 3, 2, 1], duplicates='drop')\n",
    "    rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'], q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
    "    rfm_df['M_Score'] = pd.qcut(rfm_df['Monetary'], q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
    "    \n",
    "    # Convert to numeric\n",
    "    rfm_df['R_Score'] = pd.to_numeric(rfm_df['R_Score'])\n",
    "    rfm_df['F_Score'] = pd.to_numeric(rfm_df['F_Score'])\n",
    "    rfm_df['M_Score'] = pd.to_numeric(rfm_df['M_Score'])\n",
    "    \n",
    "    # Overall RFM score\n",
    "    rfm_df['RFM_Score'] = rfm_df['R_Score'] + rfm_df['F_Score'] + rfm_df['M_Score']\n",
    "    \n",
    "    # Segment customers\n",
    "    def segment_customer(row):\n",
    "        score = row['RFM_Score']\n",
    "        if score >= 13:\n",
    "            return 'Champions'\n",
    "        elif score >= 10:\n",
    "            return 'Loyal Customers'\n",
    "        elif score >= 7:\n",
    "            return 'Potential Loyalists'\n",
    "        elif score >= 5:\n",
    "            return 'At Risk'\n",
    "        else:\n",
    "            return 'Lost'\n",
    "    \n",
    "    rfm_df['Segment'] = rfm_df.apply(segment_customer, axis=1)\n",
    "    \n",
    "    # Summary by segment\n",
    "    segment_summary = rfm_df.groupby('Segment').agg({\n",
    "        'customer_id': 'count',\n",
    "        'Recency': 'mean',\n",
    "        'Frequency': 'mean',\n",
    "        'Monetary': 'mean'\n",
    "    }).round(2)\n",
    "    segment_summary.columns = ['Customer_Count', 'Avg_Recency_Days', 'Avg_Frequency', 'Avg_Monetary']\n",
    "    \n",
    "    print(f\"\\nCustomer Segmentation:\")\n",
    "    print(segment_summary.to_string())\n",
    "    \n",
    "    # Merge with customer details\n",
    "    if not customer_df.empty and 'customer_id' in customer_df.columns:\n",
    "        rfm_with_details = rfm_df.merge(customer_df, on='customer_id', how='left')\n",
    "        return rfm_with_details\n",
    "    \n",
    "    return rfm_df\n",
    "\n",
    "# Perform RFM analysis\n",
    "rfm_results = perform_rfm_analysis(transaction_df, customer_df)\n",
    "\n",
    "if rfm_results is not None:\n",
    "    rfm_results.to_csv(outputs_path / 'customer_rfm_analysis.csv', index=False)\n",
    "    print(f\"\\nâœ“ RFM analysis saved to outputs folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46622680",
   "metadata": {},
   "source": [
    "### 4.2 Transaction Pattern Visualization\n",
    "\n",
    "**What I'm Learning**: Visual analysis helps identify trends and anomalies quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualizations, I'll need matplotlib and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Set up visualization preferences\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize transaction patterns\n",
    "if 'transaction_date' in transaction_df.columns and 'amount' in transaction_df.columns:\n",
    "    \n",
    "    transaction_df['transaction_date'] = pd.to_datetime(transaction_df['transaction_date'])\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Transaction Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Daily transaction volume\n",
    "    daily_txns = transaction_df.groupby(transaction_df['transaction_date'].dt.date)['amount'].agg(['count', 'sum'])\n",
    "    axes[0, 0].plot(daily_txns.index, daily_txns['count'], marker='o', linewidth=2)\n",
    "    axes[0, 0].set_title('Daily Transaction Volume')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Transaction Count')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Amount distribution\n",
    "    axes[0, 1].hist(transaction_df['amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Transaction Amount Distribution')\n",
    "    axes[0, 1].set_xlabel('Amount ($)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Transaction type breakdown\n",
    "    if 'transaction_type' in transaction_df.columns:\n",
    "        type_counts = transaction_df['transaction_type'].value_counts()\n",
    "        axes[1, 0].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 0].set_title('Transaction Type Distribution')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No transaction type data', ha='center', va='center')\n",
    "        axes[1, 0].set_title('Transaction Type Distribution')\n",
    "    \n",
    "    # 4. Monthly trends\n",
    "    monthly = transaction_df.groupby(transaction_df['transaction_date'].dt.to_period('M'))['amount'].sum()\n",
    "    monthly.index = monthly.index.astype(str)\n",
    "    axes[1, 1].bar(range(len(monthly)), monthly.values, alpha=0.7)\n",
    "    axes[1, 1].set_title('Monthly Transaction Volume')\n",
    "    axes[1, 1].set_xlabel('Month')\n",
    "    axes[1, 1].set_ylabel('Total Amount ($)')\n",
    "    axes[1, 1].set_xticks(range(len(monthly)))\n",
    "    axes[1, 1].set_xticklabels(monthly.index, rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outputs_path / 'transaction_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Dashboard saved to outputs folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1d74b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Data Preparation for Power BI\n",
    "\n",
    "**Learning Objective**: Create clean, aggregated datasets optimized for Power BI dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_powerbi_export():\n",
    "    \"\"\"\n",
    "    Create consolidated datasets for Power BI import\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PREPARING POWER BI EXPORT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Merge customer and transaction data\n",
    "    if not customer_df.empty and not transaction_df.empty:\n",
    "        merged_data = transaction_df.merge(\n",
    "            customer_df,\n",
    "            on='customer_id',\n",
    "            how='left',\n",
    "            suffixes=('_txn', '_cust')\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ“ Merged {len(merged_data):,} transaction records with customer data\")\n",
    "        \n",
    "        # Add calculated fields useful for Power BI\n",
    "        if 'transaction_date' in merged_data.columns:\n",
    "            merged_data['transaction_date'] = pd.to_datetime(merged_data['transaction_date'])\n",
    "            merged_data['Year'] = merged_data['transaction_date'].dt.year\n",
    "            merged_data['Month'] = merged_data['transaction_date'].dt.month\n",
    "            merged_data['Month_Name'] = merged_data['transaction_date'].dt.month_name()\n",
    "            merged_data['Quarter'] = merged_data['transaction_date'].dt.quarter\n",
    "            merged_data['Day_of_Week'] = merged_data['transaction_date'].dt.day_name()\n",
    "            merged_data['Is_Weekend'] = merged_data['transaction_date'].dt.dayofweek.isin([5, 6])\n",
    "        \n",
    "        # Create summary tables for Power BI\n",
    "        \n",
    "        # 1. Customer Summary\n",
    "        customer_summary = merged_data.groupby('customer_id').agg({\n",
    "            'amount': ['count', 'sum', 'mean', 'min', 'max'],\n",
    "            'transaction_date': ['min', 'max']\n",
    "        }).reset_index()\n",
    "        customer_summary.columns = ['customer_id', 'Total_Transactions', 'Total_Amount', \n",
    "                                    'Avg_Amount', 'Min_Amount', 'Max_Amount',\n",
    "                                    'First_Transaction', 'Last_Transaction']\n",
    "        \n",
    "        # Add customer tenure\n",
    "        customer_summary['Tenure_Days'] = (\n",
    "            customer_summary['Last_Transaction'] - customer_summary['First_Transaction']\n",
    "        ).dt.days\n",
    "        \n",
    "        print(f\"âœ“ Created customer summary: {len(customer_summary):,} records\")\n",
    "        \n",
    "        # 2. Monthly Summary\n",
    "        monthly_summary = merged_data.groupby(['Year', 'Month', 'Month_Name']).agg({\n",
    "            'transaction_id': 'count',\n",
    "            'amount': 'sum',\n",
    "            'customer_id': 'nunique'\n",
    "        }).reset_index()\n",
    "        monthly_summary.columns = ['Year', 'Month', 'Month_Name', 'Transaction_Count', \n",
    "                                   'Total_Amount', 'Unique_Customers']\n",
    "        \n",
    "        print(f\"âœ“ Created monthly summary: {len(monthly_summary):,} records\")\n",
    "        \n",
    "        # Export to Excel with multiple sheets\n",
    "        with pd.ExcelWriter(outputs_path / 'powerbi_export.xlsx', engine='openpyxl') as writer:\n",
    "            merged_data.to_excel(writer, sheet_name='Transaction_Details', index=False)\n",
    "            customer_summary.to_excel(writer, sheet_name='Customer_Summary', index=False)\n",
    "            monthly_summary.to_excel(writer, sheet_name='Monthly_Summary', index=False)\n",
    "            if rfm_results is not None:\n",
    "                rfm_results.to_excel(writer, sheet_name='RFM_Segments', index=False)\n",
    "            quality_summary.to_excel(writer, sheet_name='Data_Quality', index=False)\n",
    "        \n",
    "        print(f\"\\nâœ“ Power BI export saved: powerbi_export.xlsx\")\n",
    "        print(f\"  Contains {merged_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB of data\")\n",
    "        \n",
    "        return merged_data, customer_summary, monthly_summary\n",
    "    else:\n",
    "        print(\"âš  Cannot create export: missing customer or transaction data\")\n",
    "        return None, None, None\n",
    "\n",
    "# Prepare Power BI export\n",
    "merged_data, customer_summary, monthly_summary = prepare_powerbi_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0ba16",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Key Insights & Recommendations\n",
    "\n",
    "**What I Learned**: Always summarize findings and provide actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb40898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive insights report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Data Quality Insights\n",
    "    insights.append(\"ðŸ“Š DATA QUALITY:\")\n",
    "    avg_quality = quality_summary[quality_summary['Quality_Dimension'] == 'ðŸ“Š OVERALL QUALITY']['Score'].values[0]\n",
    "    insights.append(f\"  â€¢ Overall Data Quality Score: {avg_quality:.2f}%\")\n",
    "    \n",
    "    poor_dimensions = quality_summary[\n",
    "        (quality_summary['Status'] == 'âœ— Poor') & \n",
    "        (quality_summary['Quality_Dimension'] != 'ðŸ“Š OVERALL QUALITY')\n",
    "    ]\n",
    "    if len(poor_dimensions) > 0:\n",
    "        insights.append(f\"  â€¢ âš  {len(poor_dimensions)} quality dimension(s) need attention\")\n",
    "        for _, row in poor_dimensions.iterrows():\n",
    "            insights.append(f\"    - {row['Quality_Dimension']}: {row['Details']}\")\n",
    "    \n",
    "    # Transaction Insights\n",
    "    if not transaction_df.empty:\n",
    "        insights.append(f\"\\nðŸ’° TRANSACTION ANALYSIS:\")\n",
    "        insights.append(f\"  â€¢ Total Transactions: {len(transaction_df):,}\")\n",
    "        if 'amount' in transaction_df.columns:\n",
    "            total_amount = transaction_df['amount'].sum()\n",
    "            avg_amount = transaction_df['amount'].mean()\n",
    "            insights.append(f\"  â€¢ Total Volume: ${total_amount:,.2f}\")\n",
    "            insights.append(f\"  â€¢ Average Transaction: ${avg_amount:,.2f}\")\n",
    "    \n",
    "    # Customer Insights\n",
    "    if rfm_results is not None:\n",
    "        insights.append(f\"\\nðŸ‘¥ CUSTOMER INSIGHTS:\")\n",
    "        segment_counts = rfm_results['Segment'].value_counts()\n",
    "        insights.append(f\"  â€¢ Total Customers Analyzed: {len(rfm_results):,}\")\n",
    "        insights.append(f\"  â€¢ Top Segment: {segment_counts.index[0]} ({segment_counts.values[0]} customers)\")\n",
    "        \n",
    "        champions = len(rfm_results[rfm_results['Segment'] == 'Champions'])\n",
    "        at_risk = len(rfm_results[rfm_results['Segment'] == 'At Risk'])\n",
    "        insights.append(f\"  â€¢ Champions: {champions} customers\")\n",
    "        if at_risk > 0:\n",
    "            insights.append(f\"  â€¢ âš  At Risk: {at_risk} customers need retention efforts\")\n",
    "    \n",
    "    # Recommendations\n",
    "    insights.append(f\"\\nâœ… RECOMMENDATIONS:\")\n",
    "    insights.append(f\"  1. Address data quality issues in poor-scoring dimensions\")\n",
    "    insights.append(f\"  2. Implement automated quality monitoring\")\n",
    "    insights.append(f\"  3. Set up reconciliation processes for ongoing variance detection\")\n",
    "    if rfm_results is not None and len(rfm_results[rfm_results['Segment'] == 'At Risk']) > 0:\n",
    "        insights.append(f\"  4. Launch retention campaign for 'At Risk' customers\")\n",
    "    insights.append(f\"  5. Create Power BI dashboard for stakeholder visibility\")\n",
    "    \n",
    "    # Print all insights\n",
    "    for insight in insights:\n",
    "        print(insight)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(outputs_path / 'insights_report.txt', 'w') as f:\n",
    "        f.write('\\n'.join(insights))\n",
    "    \n",
    "    print(f\"\\nâœ“ Insights report saved to outputs folder\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate final report\n",
    "final_insights = generate_insights_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f98f7e",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What I've Accomplished\n",
    "\n",
    "In this project, I demonstrated:\n",
    "\n",
    "âœ… **Data Exploration & Profiling**\n",
    "- Loaded and profiled customer and transaction datasets\n",
    "- Identified data types, structures, and initial quality issues\n",
    "\n",
    "âœ… **Comprehensive Data Quality Assessment**\n",
    "- Completeness checks across all columns\n",
    "- Duplicate detection and uniqueness validation\n",
    "- Referential integrity verification\n",
    "- Statistical outlier detection using IQR method\n",
    "- Created quality scorecards and summaries\n",
    "\n",
    "âœ… **Reconciliation Analysis**\n",
    "- Period-over-period trend analysis\n",
    "- System-to-system comparison\n",
    "- Variance calculation and exception flagging\n",
    "- Automated exception management\n",
    "\n",
    "âœ… **Customer Analytics**\n",
    "- RFM (Recency, Frequency, Monetary) segmentation\n",
    "- Customer behavior analysis\n",
    "- Transaction pattern identification\n",
    "\n",
    "âœ… **Business Intelligence**\n",
    "- Created visualization dashboards\n",
    "- Prepared optimized datasets for Power BI\n",
    "- Generated actionable insights and recommendations\n",
    "\n",
    "âœ… **Professional Documentation**\n",
    "- Well-commented, production-ready code\n",
    "- Clear explanations of methodologies\n",
    "- Organized outputs and reports\n",
    "\n",
    "### ðŸ“ Output Files Created:\n",
    "All analysis results saved to the `outputs/` folder for easy sharing and review.\n",
    "\n",
    "### ðŸŽ¯ Next Steps:\n",
    "1. Import `powerbi_export.xlsx` into Power BI\n",
    "2. Create interactive dashboards\n",
    "3. Set up automated data pipelines\n",
    "4. Implement recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201abe48",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š Learning Resources & SQL Integration\n",
    "\n",
    "**Note**: This notebook focused on Python analysis. See `comprehensive_analysis.sql` for:\n",
    "- SQL-based data quality checks\n",
    "- Database reconciliation queries\n",
    "- Business intelligence SQL scripts\n",
    "- Performance optimization techniques\n",
    "\n",
    "**Remember**: \n",
    "- Python is great for complex analysis and machine learning\n",
    "- SQL is essential for database operations and large-scale data processing\n",
    "- Both skills are crucial for data analytics roles!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
